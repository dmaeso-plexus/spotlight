{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How I created an animation  of the embeddings during fine-tuning\n",
    "### Using Cleanlab, PCA, and Procrustes to visualize ViT fine-tuning onÂ CIFAR-10\n",
    "This notebook is part of an [article at Towards AI.](https://pub.towardsai.net/how-i-created-an-animation-of-the-embeddings-during-fine-tuning-2b8bdf49f822)\n",
    "\n",
    "Outline:\n",
    "- Preparation: Fine-tuning\n",
    "- Create Embeddings\n",
    "- Definitions of functions for Outliers, PCA and Procrustes\n",
    "- Review in Spotlight\n",
    "- Create the animation\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparation: Fine-tuning\n",
    "The fine-tuning was created based on https://huggingface.co/docs/transformers/tasks/image_classification\n",
    "- Load dataset CIFAR10\n",
    "- Load google/vit-base-patch16-224-in21k\n",
    "- Fine-tune the model on CIFAR10\n",
    "- Store a checkpoint of the fine-tuned model for each frame of the animation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Install required libraries\n",
    "!pip install transformers[torch] datasets pandas pillow cleanlab scipy matplotlib imageio renumics-spotlight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load cifar10 dataset\n",
    "from datasets import load_dataset\n",
    "\n",
    "ds = load_dataset(\"cifar10\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the label2id and id2label dicts mapping labels to index values and vice versa.\n",
    "labels = ds[\"train\"].features[\"label\"].names\n",
    "label2id, id2label = dict(), dict()\n",
    "for i, label in enumerate(labels):\n",
    "    label2id[label] = str(i)\n",
    "    id2label[str(i)] = label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load an image preprocessor for the model. This will resize the images to the correct size for the model.\n",
    "from transformers import AutoImageProcessor\n",
    "\n",
    "checkpoint = \"google/vit-base-patch16-224-in21k\"\n",
    "image_processor = AutoImageProcessor.from_pretrained(checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Take a list of PIL images and turn them to pixel values\n",
    "def transform(example_batch):\n",
    "    inputs = image_processor(\n",
    "        [x.convert(\"RGB\") for x in example_batch[\"img\"]], return_tensors=\"pt\"\n",
    "    )\n",
    "    inputs[\"label\"] = example_batch[\"label\"]\n",
    "    return inputs\n",
    "\n",
    "\n",
    "# Apply transform on the dataset\n",
    "prepared_ds = ds.with_transform(transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use a data collator to create batches\n",
    "from transformers import DefaultDataCollator\n",
    "\n",
    "data_collator = DefaultDataCollator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the pre-trained model with AutoModelForImageClassification. Specify number of labels and the label mappings.\n",
    "from transformers import AutoModelForImageClassification, TrainingArguments, Trainer\n",
    "\n",
    "model = AutoModelForImageClassification.from_pretrained(\n",
    "    checkpoint,\n",
    "    num_labels=len(labels),\n",
    "    id2label=id2label,\n",
    "    label2id=label2id,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We use an additional callback to write the loss values of time into a .csv file.\n",
    "from transformers import TrainerCallback\n",
    "\n",
    "\n",
    "class PrinterCallback(TrainerCallback):\n",
    "    def on_log(self, args, state, control, logs=None, **kwargs):\n",
    "        _ = logs.pop(\"total_flos\", None)\n",
    "        if state.is_local_process_zero:\n",
    "            if len(logs) == 3:  # skip last row\n",
    "                with open(\"log.csv\", \"a\") as f:\n",
    "                    f.write(\",\".join(map(str, logs.values())) + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup training parameters: Choose a low save_step interval for more frames in the animation\n",
    "from transformers import Trainer, TrainingArguments\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    f\"vit-base-patch16-224-in21k-ft-cifar10_highres_train\",\n",
    "    remove_unused_columns=False,\n",
    "    evaluation_strategy=\"steps\",\n",
    "    eval_steps=100,\n",
    "    save_strategy=\"steps\",\n",
    "    save_steps=20,  # the movie will be created by checkpoint save in this interval. Lower values increase the number of frames\n",
    "    per_device_train_batch_size=8,\n",
    "    gradient_accumulation_steps=4,\n",
    "    per_device_eval_batch_size=8,\n",
    "    num_train_epochs=0.04,  # use 0.04 for testing with a few frames. Use higher values for longer movies\n",
    "    warmup_ratio=0.1,\n",
    "    logging_steps=20,\n",
    "    load_best_model_at_end=False,\n",
    "    metric_for_best_model=\"accuracy\",\n",
    "    push_to_hub=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate the Trainer object and start the training\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    data_collator=data_collator,\n",
    "    train_dataset=prepared_ds[\"train\"],\n",
    "    eval_dataset=prepared_ds[\"test\"],\n",
    "    tokenizer=image_processor,\n",
    "    callbacks=[PrinterCallback],\n",
    ")\n",
    "\n",
    "# Train and save results\n",
    "train_results = trainer.train()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create embeddings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define functions to create embeddings from an individual checkpoint\n",
    "# based on https://renumics.com/next/docs/playbook/huggingface-embedding\n",
    "import datasets\n",
    "from transformers import AutoFeatureExtractor, AutoModel\n",
    "import torch\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "def extract_embeddings(model, feature_extractor, image_name=\"image\"):\n",
    "    \"\"\"Utility to compute embeddings.\"\"\"\n",
    "    device = model.device\n",
    "\n",
    "    def pp(batch):\n",
    "        images = batch[\"image\"]\n",
    "        inputs = feature_extractor(images=images, return_tensors=\"pt\").to(device)\n",
    "        embeddings = model(**inputs).last_hidden_state[:, 0].cpu()\n",
    "\n",
    "        return {\"embedding\": embeddings}\n",
    "\n",
    "    return pp\n",
    "\n",
    "\n",
    "def huggingface_embedding(\n",
    "    df,\n",
    "    image_name=\"image\",\n",
    "    inplace=False,\n",
    "    modelname=\"google/vit-base-patch16-224\",\n",
    "    batched=True,\n",
    "    batch_size=24,\n",
    "):\n",
    "    # initialize huggingface model\n",
    "    feature_extractor = AutoFeatureExtractor.from_pretrained(modelname)\n",
    "    model = AutoModel.from_pretrained(modelname, output_hidden_states=True)\n",
    "\n",
    "    # create huggingface dataset from df\n",
    "    dataset = datasets.Dataset.from_pandas(df).cast_column(image_name, datasets.Image())\n",
    "\n",
    "    # compute embedding\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    extract_fn = extract_embeddings(model.to(device), feature_extractor, image_name)\n",
    "    updated_dataset = dataset.map(extract_fn, batched=batched, batch_size=batch_size)\n",
    "\n",
    "    df_temp = updated_dataset.to_pandas()\n",
    "\n",
    "    if inplace:\n",
    "        df[\"embedding\"] = df_temp[\"embedding\"]\n",
    "        return\n",
    "\n",
    "    df_emb = pd.DataFrame()\n",
    "    df_emb[\"embedding\"] = df_temp[\"embedding\"]\n",
    "\n",
    "    return df_emb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load CIFAR-100 from Huggingface hub and convert it to a Pandas DataFrame\n",
    "import datasets\n",
    "\n",
    "ds = datasets.load_dataset(\"cifar10\", split=\"test\").prepare_for_task(\n",
    "    \"image-classification\"\n",
    ")\n",
    "df = ds.to_pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define function to get all available checkpoints as sorted folders\n",
    "import os\n",
    "import datasets\n",
    "import time\n",
    "\n",
    "\n",
    "def get_sorted_checkpoint_folders():\n",
    "    # list all subfolders of 'renumics/vit-base-patch16-224-in21k-ft-cifar10' that have checkpoint in the name\n",
    "    checkpoint_folders = [\n",
    "        x\n",
    "        for x in os.listdir(\"vit-base-patch16-224-in21k-ft-cifar10_highres_train\")\n",
    "        if \"checkpoint\" in x\n",
    "    ]\n",
    "\n",
    "    # sort the list of folders\n",
    "    sorted_checkpoint_folders = sorted(\n",
    "        checkpoint_folders, key=lambda x: int(x.split(\"-\")[-1])\n",
    "    )\n",
    "    sorted_checkpoint_folders = [\n",
    "        \"vit-base-patch16-224-in21k-ft-cifar10_highres_train\" + \"/\" + x\n",
    "        for x in sorted_checkpoint_folders\n",
    "    ]\n",
    "    return sorted_checkpoint_folders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create embeddings for each checkpoint and store them in the same folder\n",
    "for sorted_checkpoint_folder in get_sorted_checkpoint_folders():\n",
    "    # check if embedding already exists\n",
    "    if os.path.exists(sorted_checkpoint_folder + \"/embedding.pkl\"):\n",
    "        continue\n",
    "    embedding = huggingface_embedding(\n",
    "        df, modelname=sorted_checkpoint_folder, image_name=\"image\"\n",
    "    )[\"embedding\"]\n",
    "    # store in same folder\n",
    "    embedding.to_pickle(sorted_checkpoint_folder + \"/embedding.pkl\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Definitions of functions for Outliers, PCA and Procrustes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define function to extract outliers based on embeddings using cleanlab\n",
    "import io\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from cleanlab.outlier import OutOfDistribution\n",
    "\n",
    "\n",
    "def get_ood(sorted_checkpoint_folder, df):\n",
    "    embedding = pd.read_pickle(sorted_checkpoint_folder + \"/embedding.pkl\").to_list()\n",
    "    embedding_np = np.array(embedding)\n",
    "\n",
    "    ood = OutOfDistribution()\n",
    "    ood_train_feature_scores = ood.fit_score(features=embedding_np)\n",
    "    df[\"scores\"] = ood_train_feature_scores\n",
    "\n",
    "    # select row with the lowest 8 scores\n",
    "    df_ood = df.sort_values(by=[\"scores\"], ascending=True).head(8)\n",
    "    # load the 8 corresponding images\n",
    "    ood_images = [\n",
    "        (Image.open(io.BytesIO(x[\"bytes\"])).convert(\"RGB\"), l)\n",
    "        for x, l in df_ood[[\"image\", \"labels\"]].to_numpy()\n",
    "    ]\n",
    "    return ood_images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define function to generate a PCA, use procrustes to transfor the created points to given input if provided.\n",
    "# Procrustes can be used to flip, rotate and scale the points of the new frame on the old frame to stabilize the movie.\n",
    "from sklearn.decomposition import PCA\n",
    "from scipy.spatial import procrustes\n",
    "\n",
    "\n",
    "def make_pca(sorted_checkpoint_folder, pca_np):\n",
    "    embedding = pd.read_pickle(sorted_checkpoint_folder + \"/embedding.pkl\").to_list()\n",
    "    embedding_np = np.array(embedding)\n",
    "    embedding_np_flat = embedding_np.reshape(-1, 768)\n",
    "\n",
    "    pca = PCA(n_components=2)\n",
    "    pca_np_new = pca.fit_transform(embedding_np_flat)\n",
    "\n",
    "    if pca_np is None:\n",
    "        pca_np = pca_np_new\n",
    "\n",
    "    _, pca_np_new, disparity = procrustes(pca_np, pca_np_new)\n",
    "    pca_np = pca_np_new\n",
    "\n",
    "    # scale pca_np_new to be in range [-5, 5]\n",
    "    pca_np_disp = pca_np_new * 5 / np.max(np.abs(pca_np_new))\n",
    "    return pca_np_disp"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Review in Spotlight\n",
    "- load the first and latest checkpoint\n",
    "- generate embeddings, outliers and PCA\n",
    "- visualize in spotlight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load embeddings and extract outliers for first and last checkpoint and store them in the dataframe\n",
    "first, last = get_sorted_checkpoint_folders()[0], get_sorted_checkpoint_folders()[-1]\n",
    "df_ood_images_first = get_ood(first, df)\n",
    "df[\"scores_first\"] = df[\"scores\"]\n",
    "df_ood_images_last = get_ood(last, df)\n",
    "df[\"scores_last\"] = df[\"scores\"]\n",
    "del df[\"scores\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply PCA to embeddings of first and last checkpoint and store them in the dataframe\n",
    "df[\"pca_first\"] = pca_np_disp_first = make_pca(first, None).tolist()\n",
    "df[\"pca_last\"] = pca_np_disp_last = make_pca(last, pca_np_disp_first).tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add label_str column to dataframe\n",
    "df[\"label_str\"] = df[\"labels\"].apply(lambda x: ds.features[\"labels\"].int2str(x))\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize embeddings and outliers of first and last checkpoint with spotlight\n",
    "from renumics import spotlight\n",
    "\n",
    "spotlight.show(\n",
    "    df,\n",
    "    dtype={\n",
    "        \"image\": spotlight.Image,\n",
    "        \"pca_first\": spotlight.Embedding,\n",
    "        \"pca_last\": spotlight.Embedding,\n",
    "    },\n",
    "    layout=\"https://spotlight.renumics.com/resources/embeddings_pca.json\",\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create the animation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load loss from csv file into loss_df\n",
    "import pandas as pd\n",
    "\n",
    "loss_df = pd.read_csv(\"log.csv\", names=[\"loss\", \"learning_rate\", \"epoch\"])\n",
    "loss_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create an image for each checkpoint and store it next to the checkpoint\n",
    "import matplotlib.pyplot as plt\n",
    "import tqdm\n",
    "\n",
    "\n",
    "fig = plt.figure(figsize=(8, 8), dpi=200)\n",
    "all_labels = ds.to_pandas()[\"labels\"]\n",
    "pca_np_disp = None\n",
    "for i, sorted_checkpoint_folder in tqdm.tqdm(\n",
    "    enumerate(get_sorted_checkpoint_folders())\n",
    "):\n",
    "    df_ood_images = get_ood(sorted_checkpoint_folder, df)\n",
    "    pca_np_disp = make_pca(sorted_checkpoint_folder, pca_np_disp)\n",
    "\n",
    "    # prepare figure\n",
    "    fig.clf()\n",
    "    a0, a1 = fig.subplots(2, 1, gridspec_kw={\"height_ratios\": [5, 1], \"hspace\": 0.4})\n",
    "    _ = fig.suptitle(\n",
    "        \"Fine Tuning Training Step \" + str(i * 2) + \" of a Vision Transformer (ViT)\"\n",
    "    )\n",
    "\n",
    "    # setup subplot of pca points\n",
    "    a0.set_aspect(\"equal\", adjustable=\"box\")\n",
    "    a0.set_xlim(-5, 5)\n",
    "    a0.set_ylim(-5, 5)\n",
    "    _ = a0.set_xlabel(\"pca 1\")\n",
    "    _ = a0.set_ylabel(\"pca 2\")\n",
    "    _ = a0.set_title(\"PCA of embedding space\")\n",
    "\n",
    "    # add a scatter plot one by one for each label\n",
    "    for k in range(10):\n",
    "        mask = all_labels == k\n",
    "        _ = a0.scatter(pca_np_disp[mask, 0], pca_np_disp[mask, 1])\n",
    "    a0.legend(\n",
    "        labels=[ds.features[\"labels\"].int2str(x) for x in range(10)], loc=\"upper right\"\n",
    "    )\n",
    "\n",
    "    # setup subplot for loss\n",
    "    _ = a1.set_ylim(0, 3)\n",
    "    _ = a1.set_xlim(0, max(loss_df[\"epoch\"]))\n",
    "    _ = a1.set_xlabel(\"step\")\n",
    "    _ = a1.set_ylabel(\"loss\")\n",
    "    _ = a1.set_title(\"Training loss\")\n",
    "\n",
    "    # plot loss\n",
    "    loss = loss_df[\"loss\"].copy()\n",
    "    if i + 1 < len(loss):\n",
    "        loss[i + 1 :] = np.nan\n",
    "    _ = a1.plot(loss_df[\"epoch\"], loss, c=\"r\")\n",
    "\n",
    "    # add outlier images\n",
    "    for j, (img, l) in enumerate(df_ood_images):\n",
    "        newax = fig.add_axes([0.85, 0.87 - (j / 11), 0.06, 0.07], anchor=\"NE\", zorder=1)\n",
    "        newax.imshow(img)\n",
    "        newax.axis(\"off\")\n",
    "        newax.set_aspect(\"equal\", \"box\")\n",
    "        newax.set_title(\"Outlier \" + str(j) + f\" ({ds.features['labels'].int2str(l)})\")\n",
    "\n",
    "    plt.savefig(sorted_checkpoint_folder + \"/pca_dyn_procrustes_300_outlow.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import re\n",
    "import imageio\n",
    "\n",
    "# get all images from candidates\n",
    "img_paths = []\n",
    "for sorted_checkpoint_folder in get_sorted_checkpoint_folders():\n",
    "    img_paths += glob.glob(\n",
    "        sorted_checkpoint_folder + \"/pca_dyn_procrustes_300_outlow.png\"\n",
    "    )\n",
    "# sort images by number\n",
    "img_paths = sorted(img_paths, key=lambda x: int(re.findall(r\"\\d+\", x)[0]))\n",
    "\n",
    "\n",
    "with imageio.get_writer(\n",
    "    \"pca_dyn_procrustes_300_outlow.gif\", mode=\"I\", loop=0\n",
    ") as writer:\n",
    "    for filename in img_paths:\n",
    "        image = imageio.imread(filename)\n",
    "        # crop whitespace in image\n",
    "        image = image[10:-100, 110:-10]\n",
    "\n",
    "        writer.append_data(image)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[article at Towards AI](https://pub.towardsai.net/how-i-created-an-animation-of-the-embeddings-during-fine-tuning-2b8bdf49f822)Checkout the  for more details."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
